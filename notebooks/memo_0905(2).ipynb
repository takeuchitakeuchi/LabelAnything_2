{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f767c7ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d32bbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rinotsuka/code/papers/LabelAnything2/LabelAnything/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import resize\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Label Anything\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything'))\n",
    "from label_anything import LabelAnything\n",
    "from label_anything.data import get_preprocessing, utils\n",
    "from label_anything.data.transforms import PromptsProcessor\n",
    "\n",
    "# ==== å®šæ•° ====\n",
    "ANNOT_DIR = Path.cwd() / \"annotations\"\n",
    "IMAGE_DIR = Path.cwd() / \"images\"\n",
    "IMAGE_SIZE = 1024\n",
    "MASK_SIDE = 256\n",
    "CLASS_NAME_TO_ID = {\n",
    "    \"handrail\": 1, \"midrail\": 2, \"toeboard\": 3,\n",
    "    \"base_board\": 3, \"baseboard\": 3,  # COCOåã®æºã‚Œå¯¾å¿œ\n",
    "    \"æ‰‹ã™ã‚Š\": 1, \"ä¸­æ¡Ÿ\": 2, \"å·¾æœ¨\": 3,\n",
    "}\n",
    "\n",
    "# ==== å¯è¦–åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå¿…è¦ãªã‚‰ä½¿ç”¨ï¼‰ ====\n",
    "def draw_masks(img: Image.Image, masks: torch.Tensor, colors):\n",
    "    masked_image = resize(img.copy(), MASK_SIDE)\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = mask.numpy()\n",
    "        masked_image = np.where(np.repeat(mask[:, :, np.newaxis], 3, axis=2),\n",
    "                                np.asarray(colors[i % len(colors)], dtype=\"uint8\"),\n",
    "                                masked_image)\n",
    "    masked_image = masked_image.astype(np.uint8)\n",
    "    return cv2.addWeighted(np.array(resize(img, MASK_SIDE)), 0.3, masked_image, 0.7, 0)\n",
    "\n",
    "def draw_boxes(img: Image.Image, boxes: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(boxes):\n",
    "        for x1,y1,x2,y2 in cat:\n",
    "            cv2.rectangle(img, (int(x1),int(y1)), (int(x2),int(y2)), colors[i % len(colors)], 2)\n",
    "    return img\n",
    "\n",
    "def draw_points(img: Image.Image, points: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(points):\n",
    "        for x,y in cat:\n",
    "            cv2.circle(img, (int(x),int(y)), 5, colors[i % len(colors)], -1)\n",
    "    return img\n",
    "\n",
    "def draw_all(img: Image.Image, masks, boxes, points, colors):\n",
    "    segmented_image = draw_masks(img, masks, colors)\n",
    "    img = Image.fromarray(segmented_image)\n",
    "    img = resize(img, 1024)\n",
    "    img = Image.fromarray(draw_boxes(img, boxes, colors))\n",
    "    img = Image.fromarray(draw_points(img, points, colors))\n",
    "    return img\n",
    "\n",
    "def get_image(image_tensor: torch.Tensor) -> Image.Image:\n",
    "    MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "    STD  = np.array([58.395,  57.120,  57.375 ]) / 255\n",
    "    x = image_tensor.numpy()\n",
    "    x = (x * STD[:, None, None]) + MEAN[:, None, None]\n",
    "    x = (x * 255).astype(np.uint8)\n",
    "    return Image.fromarray(np.moveaxis(x, 0, -1))\n",
    "\n",
    "# ==== JSON ãƒ­ãƒ¼ãƒ€ï¼ˆCOCO / ç°¡æ˜“ï¼‰ ====\n",
    "def _mask_from_polygons(size_hw, polygons):\n",
    "    H, W = int(size_hw[0]), int(size_hw[1])\n",
    "    m = np.zeros((H, W), dtype=np.uint8)\n",
    "    for poly in polygons or []:\n",
    "        if not poly: continue\n",
    "        pts = np.array(poly, dtype=np.float32).reshape(-1, 2)\n",
    "        pts = np.round(pts).astype(np.int32)\n",
    "        cv2.fillPoly(m, [pts], 255)\n",
    "    return m\n",
    "\n",
    "def _mask_from_rle(size_hw, counts):\n",
    "    from pycocotools import mask as mask_utils  # RLEã‚’ä½¿ã‚ãªã„ãªã‚‰æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã‚‚å¯\n",
    "    rle = {\"size\": [int(size_hw[0]), int(size_hw[1])], \"counts\": counts}\n",
    "    m = mask_utils.decode(rle)\n",
    "    if m.ndim == 3: m = m[..., 0]\n",
    "    return (m.astype(np.uint8) * 255)\n",
    "\n",
    "def load_ann_any_json(json_path: Path, expected_stem: str | None = None) -> Dict[str, Dict[int, list]]:\n",
    "    out = {\"bboxes\": {}, \"points\": {}, \"masks\": {}}\n",
    "    if not json_path.exists():\n",
    "        return out\n",
    "\n",
    "    data = json.load(open(json_path, \"r\", encoding=\"utf-8\"))\n",
    "    is_coco = all(k in data for k in (\"images\",\"categories\",\"annotations\"))\n",
    "\n",
    "    if not is_coco:\n",
    "        # ç°¡æ˜“å½¢å¼: {'bboxes':{'1':[[x1,y1,x2,y2],...]}, 'points':{'2':[[x,y],...]}, 'masks':{'3':[entry,...]}}\n",
    "        for k in (\"bboxes\",\"points\"):\n",
    "            raw = data.get(k, {})\n",
    "            if isinstance(raw, dict):\n",
    "                for cid, arr in raw.items():\n",
    "                    try: cid = int(cid)\n",
    "                    except: continue\n",
    "                    if isinstance(arr, list):\n",
    "                        out[k].setdefault(cid, []).extend(arr)\n",
    "        raw_masks = data.get(\"masks\", {})\n",
    "        if isinstance(raw_masks, dict):\n",
    "            for cid, entries in raw_masks.items():\n",
    "                try: cid = int(cid)\n",
    "                except: continue\n",
    "                for entry in (entries or []):\n",
    "                    fmt = str(entry.get(\"format\",\"\")).lower()\n",
    "                    size = entry.get(\"size\", None)\n",
    "                    if not size: continue\n",
    "                    if fmt == \"polygons\":\n",
    "                        m = _mask_from_polygons(size, entry.get(\"polygons\", []))\n",
    "                        if (m>0).any(): out[\"masks\"].setdefault(cid, []).append(m)\n",
    "                    elif fmt == \"rle\":\n",
    "                        m = _mask_from_rle(size, entry.get(\"counts\"))\n",
    "                        if (m>0).any(): out[\"masks\"].setdefault(cid, []).append(m)\n",
    "        return out\n",
    "\n",
    "    # COCOå½¢å¼\n",
    "    images = data.get(\"images\", [])\n",
    "    anns   = data.get(\"annotations\", [])\n",
    "\n",
    "    # stemä¸€è‡´ â†’ 1æšã®ã¿ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ â†’ éƒ¨åˆ†ä¸€è‡´\n",
    "    img_entry = None\n",
    "    if expected_stem is not None:\n",
    "        for im in images:\n",
    "            if Path(str(im.get(\"file_name\",\"\"))).stem == expected_stem:\n",
    "                img_entry = im; break\n",
    "    if img_entry is None and len(images)==1:\n",
    "        img_entry = images[0]\n",
    "    if img_entry is None and expected_stem is not None:\n",
    "        for im in images:\n",
    "            if expected_stem in Path(str(im.get(\"file_name\",\"\"))).stem:\n",
    "                img_entry = im; break\n",
    "    if img_entry is None:\n",
    "        return out\n",
    "\n",
    "    image_id = img_entry[\"id\"]\n",
    "    H, W = int(img_entry.get(\"height\", 0)), int(img_entry.get(\"width\", 0))\n",
    "    tanns = [a for a in anns if a.get(\"image_id\")==image_id]\n",
    "\n",
    "    def add(dic, cid, v): dic.setdefault(int(cid), []).append(v)\n",
    "\n",
    "    for a in tanns:\n",
    "        cid = int(a.get(\"category_id\"))\n",
    "\n",
    "        # bbox [x,y,w,h] â†’ [x1,y1,x2,y2]\n",
    "        bb = a.get(\"bbox\")\n",
    "        if isinstance(bb, (list,tuple)) and len(bb)==4:\n",
    "            x,y,w,h = bb\n",
    "            x1,y1,x2,y2 = float(x), float(y), float(x)+float(w), float(y)+float(h)\n",
    "            if x2>x1 and y2>y1:\n",
    "                add(out[\"bboxes\"], cid, [x1,y1,x2,y2])\n",
    "\n",
    "        # keypoints â†’ pointsï¼ˆv>0ã®ã¿ï¼‰\n",
    "        kps = a.get(\"keypoints\")\n",
    "        if isinstance(kps, list) and len(kps)>=3:\n",
    "            for i in range(0,len(kps),3):\n",
    "                xk, yk, v = kps[i], kps[i+1], kps[i+2]\n",
    "                if v and xk is not None and yk is not None:\n",
    "                    add(out[\"points\"], cid, [float(xk), float(yk)])\n",
    "\n",
    "        # segmentation â†’ masksï¼ˆRLE / polygons, ç©ºé…åˆ—ã¯ç„¡è¦–ï¼‰\n",
    "        seg = a.get(\"segmentation\")\n",
    "        if seg is not None:\n",
    "            try:\n",
    "                if isinstance(seg, dict) and \"counts\" in seg and \"size\" in seg:\n",
    "                    m = _mask_from_rle(seg[\"size\"], seg[\"counts\"])\n",
    "                    if (m>0).any(): add(out[\"masks\"], cid, m)\n",
    "                elif isinstance(seg, list) and seg and isinstance(seg[0], dict) and \"counts\" in seg[0]:\n",
    "                    for r in seg:\n",
    "                        m = _mask_from_rle(r[\"size\"], r[\"counts\"])\n",
    "                        if (m>0).any(): add(out[\"masks\"], cid, m)\n",
    "                elif isinstance(seg, list) and len(seg)>0:\n",
    "                    m = _mask_from_polygons([H,W], seg)\n",
    "                    if (m>0).any(): add(out[\"masks\"], cid, m)\n",
    "            except Exception:\n",
    "                # pycocotoolsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§RLEãŒæ¥ãŸå ´åˆãªã©ã¯ç„¡è¦–ï¼ˆpolygonsã‚’æ¨å¥¨ï¼‰\n",
    "                pass\n",
    "\n",
    "    # å…¨ã‚¼ãƒ­ãƒã‚¹ã‚¯ã®æœ€çµ‚é™¤å¤–ï¼ˆä¿é™ºï¼‰\n",
    "    for cid, arrs in list(out[\"masks\"].items()):\n",
    "        out[\"masks\"][cid] = [m for m in arrs if isinstance(m, np.ndarray) and (m>0).any()]\n",
    "\n",
    "    return out\n",
    "\n",
    "def union_class_ids(dicts_per_support: List[Dict[str, Dict[int, list]]]) -> List[int]:\n",
    "    s = set()\n",
    "    for d in dicts_per_support:\n",
    "        for k in (\"bboxes\",\"points\",\"masks\"):\n",
    "            s |= set(d.get(k, {}).keys())\n",
    "    return sorted(s)\n",
    "\n",
    "# ==== å°æŠ€ï¼šç©ºãƒã‚¹ã‚¯é™¤å¤– & é‡å¿ƒç‚¹è£œå®Œ ====\n",
    "def filter_empty_masks(masks_per_img: Dict[int, List[np.ndarray]]):\n",
    "    for cid, arrs in list(masks_per_img.items()):\n",
    "        masks_per_img[cid] = [m for m in arrs if isinstance(m, np.ndarray) and m.ndim==2 and (m>0).any()]\n",
    "\n",
    "def mask_centroid(mask: np.ndarray):\n",
    "    ys, xs = np.nonzero(mask > 0)\n",
    "    if len(xs)==0: return None\n",
    "    return int(xs.mean()), int(ys.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5c7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ãƒ¢ãƒ‡ãƒ«/å‰å‡¦ç† ====\n",
    "accelerator = Accelerator(cpu=True)\n",
    "device = accelerator.device\n",
    "\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "# ====== ã“ã“ã‹ã‚‰å¤‰æ›´ï¼ˆã‚¯ã‚¨ãƒª/ã‚µãƒãƒ¼ãƒˆã‚’åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ã«ï¼‰======\n",
    "QUERY_DIR   = Path.cwd() / \"query_images\"    # â†æ–°è¦\n",
    "SUPPORT_DIR = Path.cwd() / \"support_images\"  # â†æ–°è¦\n",
    "\n",
    "def open_rgb(p: Path) -> Image.Image:\n",
    "    return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "# ã‚¯ã‚¨ãƒªç”»åƒï¼ˆ1æšæƒ³å®šï¼‰\n",
    "query_paths = sorted(\n",
    "    list(QUERY_DIR.glob(\"*.jpg\")) + list(QUERY_DIR.glob(\"*.jpeg\")) +\n",
    "    list(QUERY_DIR.glob(\"*.png\")) + list(QUERY_DIR.glob(\"*.JPG\")) +\n",
    "    list(QUERY_DIR.glob(\"*.PNG\"))\n",
    ")\n",
    "assert len(query_paths) >= 1, \"ã‚¯ã‚¨ãƒªç”»åƒã¯ query_images ã«1æšã ã‘ç½®ã„ã¦ãã ã•ã„ã€‚\"\n",
    "query_orig = open_rgb(query_paths[0])\n",
    "\n",
    "# ã‚µãƒãƒ¼ãƒˆç”»åƒï¼ˆ1æšä»¥ä¸Šï¼‰\n",
    "support_paths = sorted(\n",
    "    list(SUPPORT_DIR.glob(\"*.jpg\")) + list(SUPPORT_DIR.glob(\"*.jpeg\")) +\n",
    "    list(SUPPORT_DIR.glob(\"*.png\")) + list(SUPPORT_DIR.glob(\"*.JPG\")) +\n",
    "    list(SUPPORT_DIR.glob(\"*.PNG\"))\n",
    ")\n",
    "assert len(support_paths) >= 1, \"ã‚µãƒãƒ¼ãƒˆç”»åƒã‚’ support_images ã«1æšä»¥ä¸Šç½®ã„ã¦ãã ã•ã„ã€‚\"\n",
    "support_orig_images = [open_rgb(p) for p in support_paths]\n",
    "\n",
    "# å‰å‡¦ç†\n",
    "preprocess = get_preprocessing({\"common\": {\"custom_preprocess\": True, \"image_size\": IMAGE_SIZE}})\n",
    "query_image   = preprocess(query_orig)\n",
    "support_images = [preprocess(img) for img in support_orig_images]\n",
    "\n",
    "# å…ƒã‚µã‚¤ã‚ºï¼ˆ(W,H)ï¼‰\n",
    "support_sizes: List[Tuple[int,int]] = [img.size for img in support_orig_images]\n",
    "all_sizes: List[Tuple[int,int]] = [query_orig.size] + support_sizes\n",
    "\n",
    "prompts_processor = PromptsProcessor(\n",
    "    long_side_length=IMAGE_SIZE, masks_side_length=MASK_SIDE, custom_preprocess=True\n",
    ")\n",
    "# ====== ã“ã“ã¾ã§å¤‰æ›´ ======\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7d46bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: bboxes (5, 4, 1, 4) points (5, 4, 0, 2) masks (5, 4, 256, 256)\n",
      "flags: bbox: 15 point: 0 mask: 15 examples: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== JSON â†’ bboxes/points/masks ====  â† ã“ã“ã‚’ç½®ãæ›ãˆ\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "def _is_coco_json(path: Path) -> bool:\n",
    "    try:\n",
    "        d = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "        return isinstance(d, dict) and all(k in d for k in (\"images\", \"annotations\", \"categories\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# 1) ANNOT_DIR å†…ã‹ã‚‰ COCO å½¢å¼ã® JSON ã‚’1ã¤è¦‹ã¤ã‘ã‚‹ï¼ˆæ˜ç¤ºæŒ‡å®šãŒã‚ã‚‹ãªã‚‰ãã‚Œã§ã‚‚OKï¼‰\n",
    "# ä¾‹: annotations/dataset.json ã‚’ä½¿ã„ãŸã„ãªã‚‰ coco_json = ANNOT_DIR / \"dataset.json\"\n",
    "coco_json = None\n",
    "for cand in sorted(ANNOT_DIR.glob(\"*.json\")):\n",
    "    if _is_coco_json(cand):\n",
    "        coco_json = cand\n",
    "        break\n",
    "assert coco_json is not None, \"COCO å½¢å¼ã®æ³¨é‡ˆJSONãŒ annotations/ ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\"\n",
    "\n",
    "# 2) ã‚µãƒãƒ¼ãƒˆç”»åƒã”ã¨ã«ã€åŒã˜ COCO JSON ã‚’æ¸¡ã—ã¤ã¤ expected_stem ã§ç”»åƒã‚’ç‰¹å®š\n",
    "support_annots: List[Dict[str, Dict[int, list]]] = []\n",
    "for p in support_paths:\n",
    "    # æ­£è¦åŒ–ï¼ˆNFD/NFC å·®å¯¾ç­–ï¼‰\n",
    "    expected = unicodedata.normalize(\"NFC\", p.stem)\n",
    "    support_annots.append(load_ann_any_json(coco_json, expected_stem=expected))\n",
    "\n",
    "\n",
    "# # ==== JSON â†’ bboxes/points/masks ====\n",
    "# support_annots: List[Dict[str, Dict[int, list]]] = []\n",
    "# for p in support_paths:  # â† ãƒ•ã‚©ãƒ«ãƒ€åˆ†é›¢ã—ãŸãªã‚‰ã“ã£ã¡ï¼\n",
    "#     ann_path = (ANNOT_DIR / f\"{p.stem}.json\").resolve()\n",
    "#     support_annots.append(load_ann_any_json(ann_path, expected_stem=p.stem))\n",
    "    \n",
    "\n",
    "cat_ids = union_class_ids(support_annots) or [1,2,3]\n",
    "cat_ids = sorted(cat_ids)\n",
    "\n",
    "bboxes_list: List[Dict[int, List[List[float]]]] = []\n",
    "points_list: List[Dict[int, List[List[float]]]] = []\n",
    "masks_list : List[Dict[int, List[np.ndarray]]] = []\n",
    "\n",
    "for ann in support_annots:\n",
    "    per_b = {cid: list(ann.get(\"bboxes\", {}).get(cid, [])) for cid in cat_ids}\n",
    "    per_p = {cid: list(ann.get(\"points\", {}).get(cid, [])) for cid in cat_ids}\n",
    "    per_m = {cid: list(ann.get(\"masks\",  {}).get(cid, [])) for cid in cat_ids}\n",
    "\n",
    "    # å°æŠ€1: ç©ºãƒã‚¹ã‚¯é™¤å¤–\n",
    "    filter_empty_masks(per_m)\n",
    "\n",
    "    # å°æŠ€2: ãƒã‚¹ã‚¯ãŒã‚ã‚‹ã®ã«ãƒã‚¤ãƒ³ãƒˆãŒç„¡ã„ã‚¯ãƒ©ã‚¹ã«é‡å¿ƒç‚¹ã‚’1ã¤è£œå®Œ\n",
    "    # for cid, arrs in per_m.items():\n",
    "    #     if len(arrs)>0 and len(per_p.get(cid, []))==0:\n",
    "    #         c = mask_centroid(arrs[0])\n",
    "    #         if c is not None:\n",
    "    #             per_p.setdefault(cid, []).append(list(c))\n",
    "\n",
    "    bboxes_list.append(per_b)\n",
    "    points_list.append(per_p)\n",
    "    masks_list.append(per_m)\n",
    "\n",
    "# bbox ã‚’ LA å½¢å¼ã«å¤‰æ›\n",
    "converted_bboxes: List[Dict[int, List[List[float]]]] = []\n",
    "for img_bboxes, orig_img in zip(bboxes_list, support_orig_images):\n",
    "    out = {}\n",
    "    for cid, cat_bboxes in img_bboxes.items():\n",
    "        out[cid] = [prompts_processor.convert_bbox(b, *orig_img.size, noise=False) for b in cat_bboxes]\n",
    "    converted_bboxes.append(out)\n",
    "\n",
    "# èƒŒæ™¯ -1\n",
    "bboxes_list_bg = [{**{-1: []}, **bb} for bb in converted_bboxes]\n",
    "points_list_bg = [{**{-1: []}, **pp} for pp in points_list]\n",
    "masks_list_bg  = [{**{-1: []}, **mm} for mm in masks_list]\n",
    "cat_ids_bg = [-1] + cat_ids\n",
    "\n",
    "# numpy åŒ–ï¼ˆç©ºã§ã‚‚é…åˆ—åŒ–ï¼‰\n",
    "for i in range(len(bboxes_list_bg)):\n",
    "    for cid in cat_ids_bg:\n",
    "        bboxes_list_bg[i][cid] = np.array(bboxes_list_bg[i][cid], dtype=np.float32)\n",
    "        points_list_bg[i][cid] = np.array(points_list_bg[i][cid], dtype=np.float32)\n",
    "        # masks ã¯ ndarray ã®ãƒªã‚¹ãƒˆã®ã¾ã¾ï¼ˆLAå´ã§ 256 ã«æ•´å½¢ï¼‰\n",
    "\n",
    "# tensor åŒ–\n",
    "bboxes, flag_bboxes = utils.annotations_to_tensor(prompts_processor, bboxes_list_bg, support_sizes, utils.PromptType.BBOX)\n",
    "points, flag_points = utils.annotations_to_tensor(prompts_processor, points_list_bg, support_sizes, utils.PromptType.POINT)\n",
    "masks,  flag_masks  = utils.annotations_to_tensor(prompts_processor, masks_list_bg,  support_sizes, utils.PromptType.MASK)\n",
    "\n",
    "flag_examples = utils.flags_merge(flag_bboxes=flag_bboxes, flag_points=flag_points, flag_masks=flag_masks)\n",
    "\n",
    "# ==== æ¨è«– ====\n",
    "input_dict = {\n",
    "    utils.BatchKeys.IMAGES: torch.stack([query_image] + support_images).unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_BBOXES: bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_BBOXES:   flag_bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_POINTS: points.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_POINTS:   flag_points.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_MASKS:  masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_MASKS:    flag_masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_EXAMPLES: flag_examples.unsqueeze(0),\n",
    "    utils.BatchKeys.DIMS: torch.tensor([all_sizes], dtype=torch.int32),\n",
    "#     utils.BatchKeys.DIMS: torch.tensor([[\n",
    "#     (query_orig.height, query_orig.width),\n",
    "#     *[(img.height, img.width) for img in support_orig_images]\n",
    "# ]], dtype=torch.int32),\n",
    "}\n",
    "def dict_to_device(d, device):\n",
    "    if isinstance(d, torch.Tensor): return d.to(device)\n",
    "    if isinstance(d, dict): return {k: dict_to_device(v, device) for k,v in d.items()}\n",
    "    if isinstance(d, list): return [dict_to_device(v, device) for v in d]\n",
    "    return d\n",
    "input_dict = dict_to_device(input_dict, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = la(input_dict)\n",
    "logits = output[\"logits\"]\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# ==== ç°¡æ˜“ãƒ­ã‚° ====\n",
    "print(\"shapes:\",\n",
    "      \"bboxes\", tuple(bboxes.shape),\n",
    "      \"points\", tuple(points.shape),\n",
    "      \"masks\",  tuple(masks.shape))\n",
    "print(\"flags:\",\n",
    "      \"bbox:\",  int(flag_bboxes.sum().item()),\n",
    "      \"point:\", int(flag_points.sum().item()),\n",
    "      \"mask:\",  int(flag_masks.sum().item()),\n",
    "      \"examples:\", int(flag_examples.sum().item()))\n",
    "\n",
    "# ==== å¯è¦–åŒ–ï¼ˆä»»æ„ã€‚colorsã¯ä»»æ„ã®é…åˆ—ï¼‰ ====\n",
    "colors = [\n",
    "    (255,255,0),(255,0,0),(0,255,0),(0,0,255),\n",
    "    (255,0,255),(0,255,255),(255,165,0)\n",
    "]\n",
    "drawn_images = [\n",
    "    draw_all(get_image(img_t), img_masks, img_bboxes, img_points, colors)\n",
    "    for img_t, img_masks, img_bboxes, img_points in zip(\n",
    "        support_images, masks, bboxes, points\n",
    "    )\n",
    "]\n",
    "# Image.fromarray(drawn_images[0]).save(\"debug_support0_overlay.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0883a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] output/query_pred_colormap.png\n",
      "[saved] output/query_pred_overlay.png\n"
     ]
    }
   ],
   "source": [
    "# ===== Save predictions to ./output =====\n",
    "out_dir = Path(\"output\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# predictions: (B, H, W) ã‚’æƒ³å®šï¼ˆã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ B=1ï¼‰\n",
    "pred = predictions[0].detach().cpu().numpy().astype(np.int32)  # H x W\n",
    "\n",
    "# query ã®å…ƒç”»åƒï¼ˆäºˆæ¸¬ã¨åŒã‚µã‚¤ã‚ºã®ã¯ãšã€‚ä¸‡ãŒä¸€é•ãˆã°ãƒªã‚µã‚¤ã‚ºï¼‰\n",
    "qh, qw = query_orig.size[1], query_orig.size[0]  # (H,W)\n",
    "if pred.shape != (qh, qw):\n",
    "    pred = cv2.resize(pred, (qw, qh), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ â†’ è‰² ã®ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆèƒŒæ™¯å«ã‚ã¦ len(cat_ids)+1ï¼‰\n",
    "# idx=0: èƒŒæ™¯(-1), idx=1..C: cat_ids ã®é †\n",
    "cat_ids_with_bg = [-1] + cat_ids\n",
    "# å¥½ããªãƒ‘ãƒ¬ãƒƒãƒˆã«å¤‰æ›´å¯\n",
    "palette = np.array([\n",
    "    (0,0,0),       # èƒŒæ™¯: é»’\n",
    "    (255,255,0),   # class 1\n",
    "    (255,0,0),     # class 2\n",
    "    (0,255,0),     # class 3\n",
    "    (0,0,255),\n",
    "    (255,0,255),\n",
    "    (0,255,255),\n",
    "    (255,165,0),\n",
    "    (255,192,203),\n",
    "], dtype=np.uint8)\n",
    "\n",
    "num_needed = pred.max() + 1\n",
    "if num_needed > len(palette):\n",
    "    # ãƒ‘ãƒ¬ãƒƒãƒˆæ‹¡å¼µï¼ˆè¶³ã‚Šãªã„åˆ†ã¯å¾ªç’°ï¼‰\n",
    "    extra = np.vstack([palette[1:]] * ((num_needed // (len(palette)-1)) + 1))\n",
    "    palette = np.vstack([palette[:1], extra[:num_needed-1]])\n",
    "\n",
    "# ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ç”»åƒï¼ˆH,W,3ï¼‰\n",
    "color_map = palette[np.clip(pred, 0, len(palette)-1)]\n",
    "# ä¿å­˜ï¼ˆã‚¯ãƒ©ã‚¹è‰²ã ã‘ï¼‰\n",
    "Image.fromarray(color_map).save(out_dir / \"query_pred_colormap.png\")\n",
    "\n",
    "# ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ï¼ˆå…ƒç”»åƒã¨åŠé€æ˜åˆæˆï¼‰\n",
    "query_rgb = np.array(query_orig)\n",
    "overlay = cv2.addWeighted(query_rgb, 0.5, color_map, 0.5, 0.0)\n",
    "Image.fromarray(overlay).save(out_dir / \"query_pred_overlay.png\")\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹åˆ¥ã®2å€¤ãƒã‚¹ã‚¯ï¼ˆèƒŒæ™¯ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
    "for idx, cid in enumerate(cat_ids_with_bg):\n",
    "    if cid == -1:  # èƒŒæ™¯\n",
    "        continue\n",
    "    mask_bin = (pred == idx).astype(np.uint8) * 255\n",
    "    if mask_bin.any():  # ãã®ã‚¯ãƒ©ã‚¹ãŒ1ãƒ”ã‚¯ã‚»ãƒ«ã§ã‚‚å­˜åœ¨ã™ã‚‹æ™‚ã ã‘ä¿å­˜\n",
    "        Image.fromarray(mask_bin).save(out_dir / f\"class_{cid}_mask.png\")\n",
    "\n",
    "print(f\"[saved] {out_dir}/query_pred_colormap.png\")\n",
    "print(f\"[saved] {out_dir}/query_pred_overlay.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe56777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c45dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c23989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([1, 6, 3, 1024, 1024])\n",
      "logits_size target: (1024, 1024) or upsampled to original\n",
      "forward_sec: 168.54\n",
      "logits.shape: torch.Size([1, 4, 1528, 1120])\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "la.eval().to(device)\n",
    "\n",
    "# å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ãŒæ„å›³é€šã‚Šã®ã‚µã‚¤ã‚ºã‹ã–ã£ã¨ç¢ºèª\n",
    "print(\"images:\", input_dict[utils.BatchKeys.IMAGES].shape)      # æœŸå¾…: (1, 1+S, 3, H, W)\n",
    "print(\"logits_size target:\", (IMAGE_SIZE, IMAGE_SIZE), \"or upsampled to original\")\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    out = la(input_dict)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"forward_sec:\", round(t1 - t0, 2))\n",
    "print(\"logits.shape:\", out[\"logits\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b29c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43367814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bbc793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelAnything(\n",
       "  (model): Lam(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptImageEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 512)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 512)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 512)\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (act): ReLU()\n",
       "              (drop): Identity()\n",
       "            )\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (class_encoder): RandomMatrixEncoder()\n",
       "      (sparse_embedding_attention): AttentionMLPBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop): Identity()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (no_sparse_embedding): Embedding(1, 512)\n",
       "      (class_projector_in): Identity()\n",
       "      (class_projector_out): Identity()\n",
       "      (example_attention): AttentionMLPBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop): Identity()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (not_a_mask_embed): Embedding(1, 512)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoderLam(\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (class_mlp): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
       "          (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout): Identity()\n",
       "      )\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (act): ReLU()\n",
       "              (drop): Identity()\n",
       "            )\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (spatial_convs): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from label_anything import LabelAnything\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "la.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec7991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c40bcc95",
   "metadata": {},
   "source": [
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68e4d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tensors: 417\n",
      "['model.image_encoder.blocks.0.attn.proj.bias', 'model.image_encoder.blocks.0.attn.proj.weight', 'model.image_encoder.blocks.0.attn.qkv.bias', 'model.image_encoder.blocks.0.attn.qkv.weight', 'model.image_encoder.blocks.0.attn.rel_pos_h', 'model.image_encoder.blocks.0.attn.rel_pos_w', 'model.image_encoder.blocks.0.mlp.lin1.bias', 'model.image_encoder.blocks.0.mlp.lin1.weight', 'model.image_encoder.blocks.0.mlp.lin2.bias', 'model.image_encoder.blocks.0.mlp.lin2.weight', 'model.image_encoder.blocks.0.norm1.bias', 'model.image_encoder.blocks.0.norm1.weight', 'model.image_encoder.blocks.0.norm2.bias', 'model.image_encoder.blocks.0.norm2.weight', 'model.image_encoder.blocks.1.attn.proj.bias', 'model.image_encoder.blocks.1.attn.proj.weight', 'model.image_encoder.blocks.1.attn.qkv.bias', 'model.image_encoder.blocks.1.attn.qkv.weight', 'model.image_encoder.blocks.1.attn.rel_pos_h', 'model.image_encoder.blocks.1.attn.rel_pos_w', 'model.image_encoder.blocks.1.mlp.lin1.bias', 'model.image_encoder.blocks.1.mlp.lin1.weight', 'model.image_encoder.blocks.1.mlp.lin2.bias', 'model.image_encoder.blocks.1.mlp.lin2.weight', 'model.image_encoder.blocks.1.norm1.bias', 'model.image_encoder.blocks.1.norm1.weight', 'model.image_encoder.blocks.1.norm2.bias', 'model.image_encoder.blocks.1.norm2.weight', 'model.image_encoder.blocks.10.attn.proj.bias', 'model.image_encoder.blocks.10.attn.proj.weight', 'model.image_encoder.blocks.10.attn.qkv.bias', 'model.image_encoder.blocks.10.attn.qkv.weight', 'model.image_encoder.blocks.10.attn.rel_pos_h', 'model.image_encoder.blocks.10.attn.rel_pos_w', 'model.image_encoder.blocks.10.mlp.lin1.bias', 'model.image_encoder.blocks.10.mlp.lin1.weight', 'model.image_encoder.blocks.10.mlp.lin2.bias', 'model.image_encoder.blocks.10.mlp.lin2.weight', 'model.image_encoder.blocks.10.norm1.bias', 'model.image_encoder.blocks.10.norm1.weight', 'model.image_encoder.blocks.10.norm2.bias', 'model.image_encoder.blocks.10.norm2.weight', 'model.image_encoder.blocks.11.attn.proj.bias', 'model.image_encoder.blocks.11.attn.proj.weight', 'model.image_encoder.blocks.11.attn.qkv.bias', 'model.image_encoder.blocks.11.attn.qkv.weight', 'model.image_encoder.blocks.11.attn.rel_pos_h', 'model.image_encoder.blocks.11.attn.rel_pos_w', 'model.image_encoder.blocks.11.mlp.lin1.bias', 'model.image_encoder.blocks.11.mlp.lin1.weight']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "repo = \"pasqualedem/label_anything_sam_1024_coco\"\n",
    "\n",
    "# safetensors ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "ckpt = hf_hub_download(repo_id=repo, filename=\"model.safetensors\")\n",
    "\n",
    "# state_dict ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "sd = load_file(ckpt)\n",
    "\n",
    "# ã‚­ãƒ¼ä¸€è¦§ã‚’ç¢ºèª\n",
    "keys = list(sd.keys())\n",
    "print(\"num tensors:\", len(keys))\n",
    "print(keys[:50])  # æœ€åˆã®50å€‹ã ã‘è¡¨ç¤º\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8a696d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(\"prompt_encoder\" in k for k in keys), any(\"mask_decoder\" in k for k in keys), any(\"neck\" in k for k in keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b701e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 encoder, 130 prompt, 104 decoder, 12 neck\n"
     ]
    }
   ],
   "source": [
    "# ä¾‹ï¼šã–ã£ãã‚Šä»¶æ•°ã‚’è¦‹ã‚‹\n",
    "\n",
    "enc   = [k for k in keys if \"image_encoder\"  in k]\n",
    "prompt= [k for k in keys if \"prompt_encoder\" in k]\n",
    "dec   = [k for k in keys if \"mask_decoder\"   in k]\n",
    "neck  = [k for k in keys if \"neck\"           in k]\n",
    "print(len(enc), \"encoder,\", len(prompt), \"prompt,\", len(dec), \"decoder,\", len(neck), \"neck\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e50139",
   "metadata": {},
   "source": [
    "ãŠãŠã€ãã‚Œã„ã«å‡ºã¾ã—ãŸã­ï¼æ•°å­—ãŒã™ã¹ã¦ç‰©èªã£ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## é›†è¨ˆçµæœã®æ„å‘³\n",
    "\n",
    "* **177 encoder**\n",
    "  â†’ ViT-B/1024 (SAMãƒ™ãƒ¼ã‚¹ã® image\\_encoder) ã®å…¨ãƒ–ãƒ­ãƒƒã‚¯ï¼‹LayerNormï¼‹QKV ãªã©ã€‚\n",
    "  â†’ ã“ã‚Œã¯ **SAM ã®äº‹å‰å­¦ç¿’æ¸ˆã¿éƒ¨åˆ†**ã€‚\n",
    "\n",
    "* **130 prompt**\n",
    "  â†’ `model.prompt_encoder.*` ã®å±¤ã€‚BBox / Point / Mask ãªã©ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‚¯ã‚¨ãƒªç”»åƒã¨èåˆã™ã‚‹éƒ¨åˆ†ã€‚\n",
    "  â†’ ã“ã‚Œã¯ **COCO-20i episodic training ã§å®Ÿéš›ã«å­¦ç¿’ã•ã‚ŒãŸéƒ¨åˆ†**ã€‚\n",
    "\n",
    "* **104 decoder**\n",
    "  â†’ `model.mask_decoder.*` ã®å±¤ã€‚TwoWayTransformer ã‚„ FFN ã‚’é€šã—ã¦ã‚¯ãƒ©ã‚¹ã”ã¨ã®ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆã€‚\n",
    "  â†’ ã“ã“ã‚‚ **COCO-20i å­¦ç¿’ã§æ›´æ–°ã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†**ã€‚\n",
    "\n",
    "* **12 neck**\n",
    "  â†’ 768 (ViTå‡ºåŠ›) â†’ 512 (LAå†…éƒ¨è¡¨ç¾) ã«å†™åƒã™ã‚‹å°„å½±Convï¼‹LayerNormã€‚\n",
    "  â†’ SAM ã®å‡ºåŠ›æ¬¡å…ƒã¨ LabelAnything ã®å†…éƒ¨æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹â€œæ©‹æ¸¡ã—â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## çµè«–\n",
    "\n",
    "* `label_anything_sam_1024_coco` ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã¯\n",
    "  âœ… **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆSAM ViT-B/1024ï¼‰**\n",
    "  âœ… **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**\n",
    "  âœ… **ãƒã‚¹ã‚¯ãƒ‡ã‚³ãƒ¼ãƒ€**\n",
    "  âœ… **Neck**\n",
    "  ãŒ **ã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã‚‹**ã€‚\n",
    "* ãŸã ã— **å­¦ç¿’ã§æ›´æ–°ã•ã‚ŒãŸã®ã¯ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‹ãƒ‡ã‚³ãƒ¼ãƒ€ï¼‹Neckã€å´**ã§ã€\n",
    "  **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯å‡çµã•ã‚Œã¦ SAM ã®é‡ã¿ã‚’ãã®ã¾ã¾æŒã£ã¦ã„ã‚‹**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ ã¤ã¾ã‚Šã€ã€ŒäºŒã¤ã‚ã‚‹ï¼Ÿã€ã®ç­”ãˆã¯ **Yesã€å«ã¾ã‚Œã¦ã„ã‚‹**ã€‚\n",
    "ãã—ã¦ã€Œã©ã£ã¡ãŒå­¦ç¿’ã•ã‚ŒãŸï¼Ÿã€ã®ç­”ãˆã¯ **ãƒ‡ã‚³ãƒ¼ãƒ€å´ï¼ˆpromptï¼‹maskï¼‰ã ã‘**ã€ã§ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "ã“ã®ã‚ã¨ç¢ºèªã—ãŸã„ã®ã¯\n",
    "\n",
    "* ã€Œè‡ªåˆ†ã§ fine-tune ã™ã‚‹æ™‚ã« encoder ã‚’è§£å‡ã—ã¦å­¦ç¿’ã§ãã‚‹ã®ã‹ã€ãªã®ã‹\n",
    "* ã€Œç ”ç©¶çš„ã«ã€ã“ã® checkpoint ã‚’ã©ã†è¡¨ç¾ã™ã¹ãã‹ï¼ˆä¾‹ï¼šSAM encoder frozen, decoder trained on COCO-20iï¼‰ã€ãªã®ã‹\n",
    "\n",
    "ã©ã¡ã‚‰ã«è¿‘ã„ã§ã™ã‹ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec8b8d",
   "metadata": {},
   "source": [
    "ã„ã„è³ªå•ã§ã™ï¼\n",
    "state\\_dict ã®ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€ã‚„ã€Œã‚­ãƒ¼æ•°ã€ãŒ **è‡ªç„¶ã‹ã©ã†ã‹** ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Encoder (177 tensors)\n",
    "\n",
    "* ViT-B/16 (SAM ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€) ã®å ´åˆã€\n",
    "\n",
    "  * 12å±¤ã® Transformer block Ã— (attention, mlp, norm ã®è¤‡æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)\n",
    "  * patch embedding / positional encoding / æœ€çµ‚ã® norm\n",
    "* ã“ã‚Œã‚‰ã‚’åˆã‚ã›ã‚‹ã¨ **ã ã„ãŸã„ 150ã€œ200 å€‹ã®ã‚­ãƒ¼** ã«ãªã‚‹ã®ã¯æ™®é€šã§ã™ã€‚\n",
    "  â†’ **177** ã¯å¦¥å½“ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prompt Encoder (130 tensors)\n",
    "\n",
    "* LabelAnything ã§ã¯ã€ŒBBox / Point / Mask ã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ã€‚\n",
    "* Conv / Linear / LayerNorm / Transformer å±¤ã‚’å«ã‚€ã€‚\n",
    "* ã“ã®è¦æ¨¡æ„Ÿã§ **100å‰å¾Œã®ã‚­ãƒ¼**ãŒã‚ã£ã¦è‡ªç„¶ã€‚\n",
    "  â†’ **130** ã¨ã„ã†æ•°ã¯æƒ³å®šé€šã‚Šã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Mask Decoder (104 tensors)\n",
    "\n",
    "* TwoWayTransformerï¼ˆDepth=2, heads=8, hidden\\_dim=512ï¼‰\n",
    "* ã•ã‚‰ã« FFN, LayerNorm, Conv upsampling ãªã©ã‚’å«ã‚€ã€‚\n",
    "* ã“ã‚Œã§ **100å€‹å‰å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚­ãƒ¼**ã«ãªã‚‹ã®ã¯è‡ªç„¶ã€‚\n",
    "  â†’ **104** ã¯å¦¥å½“ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Neck (12 tensors)\n",
    "\n",
    "* Conv (1x1) â†’ LayerNorm â†’ Conv (3x3) â†’ LayerNorm\n",
    "* ã“ã‚Œã§ 2 conv Ã— 2 norm = **4ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†ã®é‡ã¿ï¼‹ãƒã‚¤ã‚¢ã‚¹ãªã©**ãŒå«ã¾ã‚Œã‚‹ã€‚\n",
    "* 1ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã« weight ã¨ bias ã®ã‚­ãƒ¼ãŒã§ãã‚‹ã®ã§ **10å‰å¾Œ**ã®ã‚­ãƒ¼æ•°ã¯è‡ªç„¶ã€‚\n",
    "  â†’ **12** ã¯ãƒ”ãƒƒã‚¿ãƒªã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ã¾ã¨ã‚\n",
    "\n",
    "* **177 (encoder) + 130 (prompt) + 104 (decoder) + 12 (neck) = 423**\n",
    "* ã‚ãªãŸã®ãƒ­ã‚°ã¯ **417 tensors** ã§ã—ãŸ â†’ è‹¥å¹²å·®ãŒã‚ã‚‹ã®ã¯\n",
    "\n",
    "  * bias ãŒç„¡ã„ Linear / Conv\n",
    "  * LayerNorm ã® affine ç„¡åŠ¹åŒ–\n",
    "    ãªã©ã®è¨­å®šã®é•ã„ã«ã‚ˆã‚‹ã‚‚ã®ã€‚\n",
    "    ã§ã‚‚æ¡æ„Ÿã¯å®Œå…¨ã«è‡ªç„¶ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "âœ… çµè«–ï¼š\n",
    "**ã“ã‚Œã‚‰ã®æ•°å€¤ã¯è‡ªç„¶ã§ã‚ã‚Šã€ã€Œã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‹ãƒ‡ã‚³ãƒ¼ãƒ€ï¼‹Neck ãŒå…¨éƒ¨å«ã¾ã‚Œã¦ã„ã‚‹ã€ã“ã¨ã‚’è£ä»˜ã‘ã‚‹ã‚‚ã®ã€‚**\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ ç¢ºèªã§ã™ãŒã€çŸ¥ã‚ŠãŸã„ã®ã¯\n",
    "\n",
    "* **ã€Œå€¤ã®æ„å‘³ï¼ã“ã®æ•°å­—ãŒã©ã‚“ãªå±¤ã«å¯¾å¿œã—ã¦ã„ã‚‹ã‹ã€** ã‚’ã‚‚ã£ã¨æ˜ã‚Šä¸‹ã’ãŸã„ã®ã‹ã€\n",
    "* ãã‚Œã¨ã‚‚ **ã€Œã“ã®æ•°ãŒå¦¥å½“ã‹ã©ã†ã‹ã€** ã®ç¢ºèªã ã‘ã§ååˆ†ã§ã™ã‹ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2237308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
