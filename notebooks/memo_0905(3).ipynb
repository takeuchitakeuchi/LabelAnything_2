{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f767c7ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d32bbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rinotsuka/code/papers/LabelAnything2/LabelAnything/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import resize\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Label Anything\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything'))\n",
    "from label_anything import LabelAnything\n",
    "from label_anything.data import get_preprocessing, utils\n",
    "from label_anything.data.transforms import PromptsProcessor\n",
    "\n",
    "# ==== 定数 ====\n",
    "ANNOT_DIR = Path.cwd() / \"annotations\"\n",
    "IMAGE_DIR = Path.cwd() / \"images\"\n",
    "IMAGE_SIZE = 1024\n",
    "MASK_SIDE = 256\n",
    "CLASS_NAME_TO_ID = {\n",
    "    \"handrail\": 1, \"midrail\": 2, \"toeboard\": 3,\n",
    "    \"base_board\": 3, \"baseboard\": 3,  # COCO名の揺れ対応\n",
    "    \"手すり\": 1, \"中桟\": 2, \"巾木\": 3,\n",
    "}\n",
    "\n",
    "# ==== 可視化ユーティリティ（必要なら使用） ====\n",
    "def draw_masks(img: Image.Image, masks: torch.Tensor, colors):\n",
    "    masked_image = resize(img.copy(), MASK_SIDE)\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = mask.numpy()\n",
    "        masked_image = np.where(np.repeat(mask[:, :, np.newaxis], 3, axis=2),\n",
    "                                np.asarray(colors[i % len(colors)], dtype=\"uint8\"),\n",
    "                                masked_image)\n",
    "    masked_image = masked_image.astype(np.uint8)\n",
    "    return cv2.addWeighted(np.array(resize(img, MASK_SIDE)), 0.3, masked_image, 0.7, 0)\n",
    "\n",
    "def draw_boxes(img: Image.Image, boxes: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(boxes):\n",
    "        for x1,y1,x2,y2 in cat:\n",
    "            cv2.rectangle(img, (int(x1),int(y1)), (int(x2),int(y2)), colors[i % len(colors)], 2)\n",
    "    return img\n",
    "\n",
    "def draw_points(img: Image.Image, points: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(points):\n",
    "        for x,y in cat:\n",
    "            cv2.circle(img, (int(x),int(y)), 5, colors[i % len(colors)], -1)\n",
    "    return img\n",
    "\n",
    "def draw_all(img: Image.Image, masks, boxes, points, colors):\n",
    "    segmented_image = draw_masks(img, masks, colors)\n",
    "    img = Image.fromarray(segmented_image)\n",
    "    img = resize(img, 1024)\n",
    "    img = Image.fromarray(draw_boxes(img, boxes, colors))\n",
    "    img = Image.fromarray(draw_points(img, points, colors))\n",
    "    return img\n",
    "\n",
    "def get_image(image_tensor: torch.Tensor) -> Image.Image:\n",
    "    MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "    STD  = np.array([58.395,  57.120,  57.375 ]) / 255\n",
    "    x = image_tensor.numpy()\n",
    "    x = (x * STD[:, None, None]) + MEAN[:, None, None]\n",
    "    x = (x * 255).astype(np.uint8)\n",
    "    return Image.fromarray(np.moveaxis(x, 0, -1))\n",
    "\n",
    "# ==== JSON ローダ（COCO / 簡易） ====\n",
    "def _mask_from_polygons(size_hw, polygons):\n",
    "    H, W = int(size_hw[0]), int(size_hw[1])\n",
    "    m = np.zeros((H, W), dtype=np.uint8)\n",
    "    for poly in polygons or []:\n",
    "        if not poly: continue\n",
    "        pts = np.array(poly, dtype=np.float32).reshape(-1, 2)\n",
    "        pts = np.round(pts).astype(np.int32)\n",
    "        cv2.fillPoly(m, [pts], 255)\n",
    "    return m\n",
    "\n",
    "def _mask_from_rle(size_hw, counts):\n",
    "    from pycocotools import mask as mask_utils  # RLEを使わないなら未インストールでも可\n",
    "    rle = {\"size\": [int(size_hw[0]), int(size_hw[1])], \"counts\": counts}\n",
    "    m = mask_utils.decode(rle)\n",
    "    if m.ndim == 3: m = m[..., 0]\n",
    "    return (m.astype(np.uint8) * 255)\n",
    "\n",
    "def load_ann_any_json(json_path: Path, expected_stem: str | None = None) -> Dict[str, Dict[int, list]]:\n",
    "    out = {\"bboxes\": {}, \"points\": {}, \"masks\": {}}\n",
    "    if not json_path.exists():\n",
    "        return out\n",
    "\n",
    "    data = json.load(open(json_path, \"r\", encoding=\"utf-8\"))\n",
    "    is_coco = all(k in data for k in (\"images\",\"categories\",\"annotations\"))\n",
    "\n",
    "    if not is_coco:\n",
    "        # 簡易形式: {'bboxes':{'1':[[x1,y1,x2,y2],...]}, 'points':{'2':[[x,y],...]}, 'masks':{'3':[entry,...]}}\n",
    "        for k in (\"bboxes\",\"points\"):\n",
    "            raw = data.get(k, {})\n",
    "            if isinstance(raw, dict):\n",
    "                for cid, arr in raw.items():\n",
    "                    try: cid = int(cid)\n",
    "                    except: continue\n",
    "                    if isinstance(arr, list):\n",
    "                        out[k].setdefault(cid, []).extend(arr)\n",
    "        raw_masks = data.get(\"masks\", {})\n",
    "        if isinstance(raw_masks, dict):\n",
    "            for cid, entries in raw_masks.items():\n",
    "                try: cid = int(cid)\n",
    "                except: continue\n",
    "                for entry in (entries or []):\n",
    "                    fmt = str(entry.get(\"format\",\"\")).lower()\n",
    "                    size = entry.get(\"size\", None)\n",
    "                    if not size: continue\n",
    "                    if fmt == \"polygons\":\n",
    "                        m = _mask_from_polygons(size, entry.get(\"polygons\", []))\n",
    "                        if (m>0).any(): out[\"masks\"].setdefault(cid, []).append(m)\n",
    "                    elif fmt == \"rle\":\n",
    "                        m = _mask_from_rle(size, entry.get(\"counts\"))\n",
    "                        if (m>0).any(): out[\"masks\"].setdefault(cid, []).append(m)\n",
    "        return out\n",
    "\n",
    "    # COCO形式\n",
    "    images = data.get(\"images\", [])\n",
    "    anns   = data.get(\"annotations\", [])\n",
    "\n",
    "    # stem一致 → 1枚のみならフォールバック → 部分一致\n",
    "    img_entry = None\n",
    "    if expected_stem is not None:\n",
    "        for im in images:\n",
    "            if Path(str(im.get(\"file_name\",\"\"))).stem == expected_stem:\n",
    "                img_entry = im; break\n",
    "    if img_entry is None and len(images)==1:\n",
    "        img_entry = images[0]\n",
    "    if img_entry is None and expected_stem is not None:\n",
    "        for im in images:\n",
    "            if expected_stem in Path(str(im.get(\"file_name\",\"\"))).stem:\n",
    "                img_entry = im; break\n",
    "    if img_entry is None:\n",
    "        return out\n",
    "\n",
    "    image_id = img_entry[\"id\"]\n",
    "    H, W = int(img_entry.get(\"height\", 0)), int(img_entry.get(\"width\", 0))\n",
    "    tanns = [a for a in anns if a.get(\"image_id\")==image_id]\n",
    "\n",
    "    def add(dic, cid, v): dic.setdefault(int(cid), []).append(v)\n",
    "\n",
    "    for a in tanns:\n",
    "        cid = int(a.get(\"category_id\"))\n",
    "\n",
    "        # bbox [x,y,w,h] → [x1,y1,x2,y2]\n",
    "        bb = a.get(\"bbox\")\n",
    "        if isinstance(bb, (list,tuple)) and len(bb)==4:\n",
    "            x,y,w,h = bb\n",
    "            x1,y1,x2,y2 = float(x), float(y), float(x)+float(w), float(y)+float(h)\n",
    "            if x2>x1 and y2>y1:\n",
    "                add(out[\"bboxes\"], cid, [x1,y1,x2,y2])\n",
    "\n",
    "        # keypoints → points（v>0のみ）\n",
    "        kps = a.get(\"keypoints\")\n",
    "        if isinstance(kps, list) and len(kps)>=3:\n",
    "            for i in range(0,len(kps),3):\n",
    "                xk, yk, v = kps[i], kps[i+1], kps[i+2]\n",
    "                if v and xk is not None and yk is not None:\n",
    "                    add(out[\"points\"], cid, [float(xk), float(yk)])\n",
    "\n",
    "        # segmentation → masks（RLE / polygons, 空配列は無視）\n",
    "        seg = a.get(\"segmentation\")\n",
    "        if seg is not None:\n",
    "            try:\n",
    "                if isinstance(seg, dict) and \"counts\" in seg and \"size\" in seg:\n",
    "                    m = _mask_from_rle(seg[\"size\"], seg[\"counts\"])\n",
    "                    if (m>0).any(): add(out[\"masks\"], cid, m)\n",
    "                elif isinstance(seg, list) and seg and isinstance(seg[0], dict) and \"counts\" in seg[0]:\n",
    "                    for r in seg:\n",
    "                        m = _mask_from_rle(r[\"size\"], r[\"counts\"])\n",
    "                        if (m>0).any(): add(out[\"masks\"], cid, m)\n",
    "                elif isinstance(seg, list) and len(seg)>0:\n",
    "                    m = _mask_from_polygons([H,W], seg)\n",
    "                    if (m>0).any(): add(out[\"masks\"], cid, m)\n",
    "            except Exception:\n",
    "                # pycocotools未インストールでRLEが来た場合などは無視（polygonsを推奨）\n",
    "                pass\n",
    "\n",
    "    # 全ゼロマスクの最終除外（保険）\n",
    "    for cid, arrs in list(out[\"masks\"].items()):\n",
    "        out[\"masks\"][cid] = [m for m in arrs if isinstance(m, np.ndarray) and (m>0).any()]\n",
    "\n",
    "    return out\n",
    "\n",
    "def union_class_ids(dicts_per_support: List[Dict[str, Dict[int, list]]]) -> List[int]:\n",
    "    s = set()\n",
    "    for d in dicts_per_support:\n",
    "        for k in (\"bboxes\",\"points\",\"masks\"):\n",
    "            s |= set(d.get(k, {}).keys())\n",
    "    return sorted(s)\n",
    "\n",
    "# ==== 小技：空マスク除外 & 重心点補完 ====\n",
    "def filter_empty_masks(masks_per_img: Dict[int, List[np.ndarray]]):\n",
    "    for cid, arrs in list(masks_per_img.items()):\n",
    "        masks_per_img[cid] = [m for m in arrs if isinstance(m, np.ndarray) and m.ndim==2 and (m>0).any()]\n",
    "\n",
    "def mask_centroid(mask: np.ndarray):\n",
    "    ys, xs = np.nonzero(mask > 0)\n",
    "    if len(xs)==0: return None\n",
    "    return int(xs.mean()), int(ys.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5c7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== モデル/前処理 ====\n",
    "accelerator = Accelerator(cpu=True)\n",
    "device = accelerator.device\n",
    "\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "# ====== ここから変更（クエリ/サポートを別フォルダに）======\n",
    "QUERY_DIR   = Path.cwd() / \"query_images\"    # ←新規\n",
    "SUPPORT_DIR = Path.cwd() / \"support_images\"  # ←新規\n",
    "\n",
    "def open_rgb(p: Path) -> Image.Image:\n",
    "    return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "# クエリ画像（1枚想定）\n",
    "query_paths = sorted(\n",
    "    list(QUERY_DIR.glob(\"*.jpg\")) + list(QUERY_DIR.glob(\"*.jpeg\")) +\n",
    "    list(QUERY_DIR.glob(\"*.png\")) + list(QUERY_DIR.glob(\"*.JPG\")) +\n",
    "    list(QUERY_DIR.glob(\"*.PNG\"))\n",
    ")\n",
    "assert len(query_paths) >= 1, \"クエリ画像は query_images に1枚だけ置いてください。\"\n",
    "query_orig = open_rgb(query_paths[0])\n",
    "\n",
    "# サポート画像（1枚以上）\n",
    "support_paths = sorted(\n",
    "    list(SUPPORT_DIR.glob(\"*.jpg\")) + list(SUPPORT_DIR.glob(\"*.jpeg\")) +\n",
    "    list(SUPPORT_DIR.glob(\"*.png\")) + list(SUPPORT_DIR.glob(\"*.JPG\")) +\n",
    "    list(SUPPORT_DIR.glob(\"*.PNG\"))\n",
    ")\n",
    "assert len(support_paths) >= 1, \"サポート画像を support_images に1枚以上置いてください。\"\n",
    "support_orig_images = [open_rgb(p) for p in support_paths]\n",
    "\n",
    "# 前処理\n",
    "preprocess = get_preprocessing({\"common\": {\"custom_preprocess\": True, \"image_size\": IMAGE_SIZE}})\n",
    "query_image   = preprocess(query_orig)\n",
    "support_images = [preprocess(img) for img in support_orig_images]\n",
    "\n",
    "# 元サイズ（(W,H)）\n",
    "support_sizes: List[Tuple[int,int]] = [img.size for img in support_orig_images]\n",
    "all_sizes: List[Tuple[int,int]] = [query_orig.size] + support_sizes\n",
    "\n",
    "prompts_processor = PromptsProcessor(\n",
    "    long_side_length=IMAGE_SIZE, masks_side_length=MASK_SIDE, custom_preprocess=True\n",
    ")\n",
    "# ====== ここまで変更 ======\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7d46bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: bboxes (5, 4, 1, 4) points (5, 4, 0, 2) masks (5, 4, 256, 256)\n",
      "flags: bbox: 15 point: 0 mask: 15 examples: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== JSON → bboxes/points/masks ====  ← ここを置き換え\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "def _is_coco_json(path: Path) -> bool:\n",
    "    try:\n",
    "        d = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "        return isinstance(d, dict) and all(k in d for k in (\"images\", \"annotations\", \"categories\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# 1) ANNOT_DIR 内から COCO 形式の JSON を1つ見つける（明示指定があるならそれでもOK）\n",
    "# 例: annotations/dataset.json を使いたいなら coco_json = ANNOT_DIR / \"dataset.json\"\n",
    "coco_json = None\n",
    "for cand in sorted(ANNOT_DIR.glob(\"*.json\")):\n",
    "    if _is_coco_json(cand):\n",
    "        coco_json = cand\n",
    "        break\n",
    "assert coco_json is not None, \"COCO 形式の注釈JSONが annotations/ に見つかりませんでした。\"\n",
    "\n",
    "# 2) サポート画像ごとに、同じ COCO JSON を渡しつつ expected_stem で画像を特定\n",
    "support_annots: List[Dict[str, Dict[int, list]]] = []\n",
    "for p in support_paths:\n",
    "    # 正規化（NFD/NFC 差対策）\n",
    "    expected = unicodedata.normalize(\"NFC\", p.stem)\n",
    "    support_annots.append(load_ann_any_json(coco_json, expected_stem=expected))\n",
    "\n",
    "\n",
    "# # ==== JSON → bboxes/points/masks ====\n",
    "# support_annots: List[Dict[str, Dict[int, list]]] = []\n",
    "# for p in support_paths:  # ← フォルダ分離したならこっち！\n",
    "#     ann_path = (ANNOT_DIR / f\"{p.stem}.json\").resolve()\n",
    "#     support_annots.append(load_ann_any_json(ann_path, expected_stem=p.stem))\n",
    "    \n",
    "\n",
    "cat_ids = union_class_ids(support_annots) or [1,2,3]\n",
    "cat_ids = sorted(cat_ids)\n",
    "\n",
    "bboxes_list: List[Dict[int, List[List[float]]]] = []\n",
    "points_list: List[Dict[int, List[List[float]]]] = []\n",
    "masks_list : List[Dict[int, List[np.ndarray]]] = []\n",
    "\n",
    "for ann in support_annots:\n",
    "    per_b = {cid: list(ann.get(\"bboxes\", {}).get(cid, [])) for cid in cat_ids}\n",
    "    per_p = {cid: list(ann.get(\"points\", {}).get(cid, [])) for cid in cat_ids}\n",
    "    per_m = {cid: list(ann.get(\"masks\",  {}).get(cid, [])) for cid in cat_ids}\n",
    "\n",
    "    # 小技1: 空マスク除外\n",
    "    filter_empty_masks(per_m)\n",
    "\n",
    "    # 小技2: マスクがあるのにポイントが無いクラスに重心点を1つ補完\n",
    "    # for cid, arrs in per_m.items():\n",
    "    #     if len(arrs)>0 and len(per_p.get(cid, []))==0:\n",
    "    #         c = mask_centroid(arrs[0])\n",
    "    #         if c is not None:\n",
    "    #             per_p.setdefault(cid, []).append(list(c))\n",
    "\n",
    "    bboxes_list.append(per_b)\n",
    "    points_list.append(per_p)\n",
    "    masks_list.append(per_m)\n",
    "\n",
    "# bbox を LA 形式に変換\n",
    "converted_bboxes: List[Dict[int, List[List[float]]]] = []\n",
    "for img_bboxes, orig_img in zip(bboxes_list, support_orig_images):\n",
    "    out = {}\n",
    "    for cid, cat_bboxes in img_bboxes.items():\n",
    "        out[cid] = [prompts_processor.convert_bbox(b, *orig_img.size, noise=False) for b in cat_bboxes]\n",
    "    converted_bboxes.append(out)\n",
    "\n",
    "# 背景 -1\n",
    "bboxes_list_bg = [{**{-1: []}, **bb} for bb in converted_bboxes]\n",
    "points_list_bg = [{**{-1: []}, **pp} for pp in points_list]\n",
    "masks_list_bg  = [{**{-1: []}, **mm} for mm in masks_list]\n",
    "cat_ids_bg = [-1] + cat_ids\n",
    "\n",
    "# numpy 化（空でも配列化）\n",
    "for i in range(len(bboxes_list_bg)):\n",
    "    for cid in cat_ids_bg:\n",
    "        bboxes_list_bg[i][cid] = np.array(bboxes_list_bg[i][cid], dtype=np.float32)\n",
    "        points_list_bg[i][cid] = np.array(points_list_bg[i][cid], dtype=np.float32)\n",
    "        # masks は ndarray のリストのまま（LA側で 256 に整形）\n",
    "\n",
    "# tensor 化\n",
    "bboxes, flag_bboxes = utils.annotations_to_tensor(prompts_processor, bboxes_list_bg, support_sizes, utils.PromptType.BBOX)\n",
    "points, flag_points = utils.annotations_to_tensor(prompts_processor, points_list_bg, support_sizes, utils.PromptType.POINT)\n",
    "masks,  flag_masks  = utils.annotations_to_tensor(prompts_processor, masks_list_bg,  support_sizes, utils.PromptType.MASK)\n",
    "\n",
    "flag_examples = utils.flags_merge(flag_bboxes=flag_bboxes, flag_points=flag_points, flag_masks=flag_masks)\n",
    "\n",
    "# ==== 推論 ====\n",
    "input_dict = {\n",
    "    utils.BatchKeys.IMAGES: torch.stack([query_image] + support_images).unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_BBOXES: bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_BBOXES:   flag_bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_POINTS: points.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_POINTS:   flag_points.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_MASKS:  masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_MASKS:    flag_masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_EXAMPLES: flag_examples.unsqueeze(0),\n",
    "    utils.BatchKeys.DIMS: torch.tensor([all_sizes], dtype=torch.int32),\n",
    "#     utils.BatchKeys.DIMS: torch.tensor([[\n",
    "#     (query_orig.height, query_orig.width),\n",
    "#     *[(img.height, img.width) for img in support_orig_images]\n",
    "# ]], dtype=torch.int32),\n",
    "}\n",
    "def dict_to_device(d, device):\n",
    "    if isinstance(d, torch.Tensor): return d.to(device)\n",
    "    if isinstance(d, dict): return {k: dict_to_device(v, device) for k,v in d.items()}\n",
    "    if isinstance(d, list): return [dict_to_device(v, device) for v in d]\n",
    "    return d\n",
    "input_dict = dict_to_device(input_dict, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = la(input_dict)\n",
    "logits = output[\"logits\"]\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# ==== 簡易ログ ====\n",
    "print(\"shapes:\",\n",
    "      \"bboxes\", tuple(bboxes.shape),\n",
    "      \"points\", tuple(points.shape),\n",
    "      \"masks\",  tuple(masks.shape))\n",
    "print(\"flags:\",\n",
    "      \"bbox:\",  int(flag_bboxes.sum().item()),\n",
    "      \"point:\", int(flag_points.sum().item()),\n",
    "      \"mask:\",  int(flag_masks.sum().item()),\n",
    "      \"examples:\", int(flag_examples.sum().item()))\n",
    "\n",
    "# ==== 可視化（任意。colorsは任意の配列） ====\n",
    "colors = [\n",
    "    (255,255,0),(255,0,0),(0,255,0),(0,0,255),\n",
    "    (255,0,255),(0,255,255),(255,165,0)\n",
    "]\n",
    "drawn_images = [\n",
    "    draw_all(get_image(img_t), img_masks, img_bboxes, img_points, colors)\n",
    "    for img_t, img_masks, img_bboxes, img_points in zip(\n",
    "        support_images, masks, bboxes, points\n",
    "    )\n",
    "]\n",
    "# Image.fromarray(drawn_images[0]).save(\"debug_support0_overlay.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0883a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] output/query_pred_colormap.png\n",
      "[saved] output/query_pred_overlay.png\n"
     ]
    }
   ],
   "source": [
    "# ===== Save predictions to ./output =====\n",
    "out_dir = Path(\"output\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# predictions: (B, H, W) を想定（このスクリプトでは B=1）\n",
    "pred = predictions[0].detach().cpu().numpy().astype(np.int32)  # H x W\n",
    "\n",
    "# query の元画像（予測と同サイズのはず。万が一違えばリサイズ）\n",
    "qh, qw = query_orig.size[1], query_orig.size[0]  # (H,W)\n",
    "if pred.shape != (qh, qw):\n",
    "    pred = cv2.resize(pred, (qw, qh), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# インデックス → 色 のルックアップ（背景含めて len(cat_ids)+1）\n",
    "# idx=0: 背景(-1), idx=1..C: cat_ids の順\n",
    "cat_ids_with_bg = [-1] + cat_ids\n",
    "# 好きなパレットに変更可\n",
    "palette = np.array([\n",
    "    (0,0,0),       # 背景: 黒\n",
    "    (255,255,0),   # class 1\n",
    "    (255,0,0),     # class 2\n",
    "    (0,255,0),     # class 3\n",
    "    (0,0,255),\n",
    "    (255,0,255),\n",
    "    (0,255,255),\n",
    "    (255,165,0),\n",
    "    (255,192,203),\n",
    "], dtype=np.uint8)\n",
    "\n",
    "num_needed = pred.max() + 1\n",
    "if num_needed > len(palette):\n",
    "    # パレット拡張（足りない分は循環）\n",
    "    extra = np.vstack([palette[1:]] * ((num_needed // (len(palette)-1)) + 1))\n",
    "    palette = np.vstack([palette[:1], extra[:num_needed-1]])\n",
    "\n",
    "# カラーマップ画像（H,W,3）\n",
    "color_map = palette[np.clip(pred, 0, len(palette)-1)]\n",
    "# 保存（クラス色だけ）\n",
    "Image.fromarray(color_map).save(out_dir / \"query_pred_colormap.png\")\n",
    "\n",
    "# オーバーレイ（元画像と半透明合成）\n",
    "query_rgb = np.array(query_orig)\n",
    "overlay = cv2.addWeighted(query_rgb, 0.5, color_map, 0.5, 0.0)\n",
    "Image.fromarray(overlay).save(out_dir / \"query_pred_overlay.png\")\n",
    "\n",
    "# クラス別の2値マスク（背景はスキップ）\n",
    "for idx, cid in enumerate(cat_ids_with_bg):\n",
    "    if cid == -1:  # 背景\n",
    "        continue\n",
    "    mask_bin = (pred == idx).astype(np.uint8) * 255\n",
    "    if mask_bin.any():  # そのクラスが1ピクセルでも存在する時だけ保存\n",
    "        Image.fromarray(mask_bin).save(out_dir / f\"class_{cid}_mask.png\")\n",
    "\n",
    "print(f\"[saved] {out_dir}/query_pred_colormap.png\")\n",
    "print(f\"[saved] {out_dir}/query_pred_overlay.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe56777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c45dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c23989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([1, 6, 3, 1024, 1024])\n",
      "logits_size target: (1024, 1024) or upsampled to original\n",
      "forward_sec: 168.54\n",
      "logits.shape: torch.Size([1, 4, 1528, 1120])\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "la.eval().to(device)\n",
    "\n",
    "# 入力テンソルが意図通りのサイズかざっと確認\n",
    "print(\"images:\", input_dict[utils.BatchKeys.IMAGES].shape)      # 期待: (1, 1+S, 3, H, W)\n",
    "print(\"logits_size target:\", (IMAGE_SIZE, IMAGE_SIZE), \"or upsampled to original\")\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    out = la(input_dict)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"forward_sec:\", round(t1 - t0, 2))\n",
    "print(\"logits.shape:\", out[\"logits\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b29c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43367814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bbc793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelAnything(\n",
       "  (model): Lam(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptImageEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 512)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 512)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 512)\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (act): ReLU()\n",
       "              (drop): Identity()\n",
       "            )\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (class_encoder): RandomMatrixEncoder()\n",
       "      (sparse_embedding_attention): AttentionMLPBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop): Identity()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (no_sparse_embedding): Embedding(1, 512)\n",
       "      (class_projector_in): Identity()\n",
       "      (class_projector_out): Identity()\n",
       "      (example_attention): AttentionMLPBlock(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop): Identity()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (not_a_mask_embed): Embedding(1, 512)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoderLam(\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (class_mlp): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
       "          (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "        )\n",
       "        (dropout): Identity()\n",
       "      )\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (act): ReLU()\n",
       "              (drop): Identity()\n",
       "            )\n",
       "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): Attention(\n",
       "              (drop): Identity()\n",
       "              (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): Attention(\n",
       "          (drop): Identity()\n",
       "          (q_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (spatial_convs): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from label_anything import LabelAnything\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "la.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec7991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c40bcc95",
   "metadata": {},
   "source": [
    "# チェックポイント確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68e4d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tensors: 417\n",
      "['model.image_encoder.blocks.0.attn.proj.bias', 'model.image_encoder.blocks.0.attn.proj.weight', 'model.image_encoder.blocks.0.attn.qkv.bias', 'model.image_encoder.blocks.0.attn.qkv.weight', 'model.image_encoder.blocks.0.attn.rel_pos_h', 'model.image_encoder.blocks.0.attn.rel_pos_w', 'model.image_encoder.blocks.0.mlp.lin1.bias', 'model.image_encoder.blocks.0.mlp.lin1.weight', 'model.image_encoder.blocks.0.mlp.lin2.bias', 'model.image_encoder.blocks.0.mlp.lin2.weight', 'model.image_encoder.blocks.0.norm1.bias', 'model.image_encoder.blocks.0.norm1.weight', 'model.image_encoder.blocks.0.norm2.bias', 'model.image_encoder.blocks.0.norm2.weight', 'model.image_encoder.blocks.1.attn.proj.bias', 'model.image_encoder.blocks.1.attn.proj.weight', 'model.image_encoder.blocks.1.attn.qkv.bias', 'model.image_encoder.blocks.1.attn.qkv.weight', 'model.image_encoder.blocks.1.attn.rel_pos_h', 'model.image_encoder.blocks.1.attn.rel_pos_w', 'model.image_encoder.blocks.1.mlp.lin1.bias', 'model.image_encoder.blocks.1.mlp.lin1.weight', 'model.image_encoder.blocks.1.mlp.lin2.bias', 'model.image_encoder.blocks.1.mlp.lin2.weight', 'model.image_encoder.blocks.1.norm1.bias', 'model.image_encoder.blocks.1.norm1.weight', 'model.image_encoder.blocks.1.norm2.bias', 'model.image_encoder.blocks.1.norm2.weight', 'model.image_encoder.blocks.10.attn.proj.bias', 'model.image_encoder.blocks.10.attn.proj.weight', 'model.image_encoder.blocks.10.attn.qkv.bias', 'model.image_encoder.blocks.10.attn.qkv.weight', 'model.image_encoder.blocks.10.attn.rel_pos_h', 'model.image_encoder.blocks.10.attn.rel_pos_w', 'model.image_encoder.blocks.10.mlp.lin1.bias', 'model.image_encoder.blocks.10.mlp.lin1.weight', 'model.image_encoder.blocks.10.mlp.lin2.bias', 'model.image_encoder.blocks.10.mlp.lin2.weight', 'model.image_encoder.blocks.10.norm1.bias', 'model.image_encoder.blocks.10.norm1.weight', 'model.image_encoder.blocks.10.norm2.bias', 'model.image_encoder.blocks.10.norm2.weight', 'model.image_encoder.blocks.11.attn.proj.bias', 'model.image_encoder.blocks.11.attn.proj.weight', 'model.image_encoder.blocks.11.attn.qkv.bias', 'model.image_encoder.blocks.11.attn.qkv.weight', 'model.image_encoder.blocks.11.attn.rel_pos_h', 'model.image_encoder.blocks.11.attn.rel_pos_w', 'model.image_encoder.blocks.11.mlp.lin1.bias', 'model.image_encoder.blocks.11.mlp.lin1.weight']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "repo = \"pasqualedem/label_anything_sam_1024_coco\"\n",
    "\n",
    "# safetensors のファイルパスを取得\n",
    "ckpt = hf_hub_download(repo_id=repo, filename=\"model.safetensors\")\n",
    "\n",
    "# state_dict をロード\n",
    "sd = load_file(ckpt)\n",
    "\n",
    "# キー一覧を確認\n",
    "keys = list(sd.keys())\n",
    "print(\"num tensors:\", len(keys))\n",
    "print(keys[:50])  # 最初の50個だけ表示\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8a696d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(\"prompt_encoder\" in k for k in keys), any(\"mask_decoder\" in k for k in keys), any(\"neck\" in k for k in keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b701e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 encoder, 130 prompt, 104 decoder, 12 neck\n"
     ]
    }
   ],
   "source": [
    "# 例：ざっくり件数を見る\n",
    "\n",
    "enc   = [k for k in keys if \"image_encoder\"  in k]\n",
    "prompt= [k for k in keys if \"prompt_encoder\" in k]\n",
    "dec   = [k for k in keys if \"mask_decoder\"   in k]\n",
    "neck  = [k for k in keys if \"neck\"           in k]\n",
    "print(len(enc), \"encoder,\", len(prompt), \"prompt,\", len(dec), \"decoder,\", len(neck), \"neck\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e50139",
   "metadata": {},
   "source": [
    "おお、きれいに出ましたね！数字がすべて物語っています。\n",
    "\n",
    "---\n",
    "\n",
    "## 集計結果の意味\n",
    "\n",
    "* **177 encoder**\n",
    "  → ViT-B/1024 (SAMベースの image\\_encoder) の全ブロック＋LayerNorm＋QKV など。\n",
    "  → これは **SAM の事前学習済み部分**。\n",
    "\n",
    "* **130 prompt**\n",
    "  → `model.prompt_encoder.*` の層。BBox / Point / Mask などをエンコードしてクエリ画像と融合する部分。\n",
    "  → これは **COCO-20i episodic training で実際に学習された部分**。\n",
    "\n",
    "* **104 decoder**\n",
    "  → `model.mask_decoder.*` の層。TwoWayTransformer や FFN を通してクラスごとのマスクを生成。\n",
    "  → ここも **COCO-20i 学習で更新されている部分**。\n",
    "\n",
    "* **12 neck**\n",
    "  → 768 (ViT出力) → 512 (LA内部表現) に写像する射影Conv＋LayerNorm。\n",
    "  → SAM の出力次元と LabelAnything の内部次元を合わせる“橋渡し”。\n",
    "\n",
    "---\n",
    "\n",
    "## 結論\n",
    "\n",
    "* `label_anything_sam_1024_coco` のチェックポイントには\n",
    "  ✅ **エンコーダ（SAM ViT-B/1024）**\n",
    "  ✅ **プロンプトエンコーダ**\n",
    "  ✅ **マスクデコーダ**\n",
    "  ✅ **Neck**\n",
    "  が **すべて含まれている**。\n",
    "* ただし **学習で更新されたのは「プロンプト＋デコーダ＋Neck」側**で、\n",
    "  **エンコーダは凍結されて SAM の重みをそのまま持っている**。\n",
    "\n",
    "---\n",
    "\n",
    "👉 つまり、「二つある？」の答えは **Yes、含まれている**。\n",
    "そして「どっちが学習された？」の答えは **デコーダ側（prompt＋mask）だけ**、です。\n",
    "\n",
    "---\n",
    "\n",
    "このあと確認したいのは\n",
    "\n",
    "* 「自分で fine-tune する時に encoder を解凍して学習できるのか」なのか\n",
    "* 「研究的に、この checkpoint をどう表現すべきか（例：SAM encoder frozen, decoder trained on COCO-20i）」なのか\n",
    "\n",
    "どちらに近いですか？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec8b8d",
   "metadata": {},
   "source": [
    "いい質問です！\n",
    "state\\_dict の「パラメータ数」や「キー数」が **自然かどうか** を見ていきましょう。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Encoder (177 tensors)\n",
    "\n",
    "* ViT-B/16 (SAM のエンコーダ) の場合、\n",
    "\n",
    "  * 12層の Transformer block × (attention, mlp, norm の複数パラメータ)\n",
    "  * patch embedding / positional encoding / 最終の norm\n",
    "* これらを合わせると **だいたい 150〜200 個のキー** になるのは普通です。\n",
    "  → **177** は妥当。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prompt Encoder (130 tensors)\n",
    "\n",
    "* LabelAnything では「BBox / Point / Mask を埋め込みに変換するモジュール」。\n",
    "* Conv / Linear / LayerNorm / Transformer 層を含む。\n",
    "* この規模感で **100前後のキー**があって自然。\n",
    "  → **130** という数は想定通り。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Mask Decoder (104 tensors)\n",
    "\n",
    "* TwoWayTransformer（Depth=2, heads=8, hidden\\_dim=512）\n",
    "* さらに FFN, LayerNorm, Conv upsampling などを含む。\n",
    "* これで **100個前後のパラメータキー**になるのは自然。\n",
    "  → **104** は妥当。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Neck (12 tensors)\n",
    "\n",
    "* Conv (1x1) → LayerNorm → Conv (3x3) → LayerNorm\n",
    "* これで 2 conv × 2 norm = **4レイヤー分の重み＋バイアスなど**が含まれる。\n",
    "* 1レイヤーごとに weight と bias のキーができるので **10前後**のキー数は自然。\n",
    "  → **12** はピッタリ。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. まとめ\n",
    "\n",
    "* **177 (encoder) + 130 (prompt) + 104 (decoder) + 12 (neck) = 423**\n",
    "* あなたのログは **417 tensors** でした → 若干差があるのは\n",
    "\n",
    "  * bias が無い Linear / Conv\n",
    "  * LayerNorm の affine 無効化\n",
    "    などの設定の違いによるもの。\n",
    "    でも桁感は完全に自然。\n",
    "\n",
    "---\n",
    "\n",
    "✅ 結論：\n",
    "**これらの数値は自然であり、「エンコーダ＋プロンプト＋デコーダ＋Neck が全部含まれている」ことを裏付けるもの。**\n",
    "\n",
    "---\n",
    "\n",
    "👉 確認ですが、知りたいのは\n",
    "\n",
    "* **「値の意味＝この数字がどんな層に対応しているか」** をもっと掘り下げたいのか、\n",
    "* それとも **「この数が妥当かどうか」** の確認だけで十分ですか？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2237308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
